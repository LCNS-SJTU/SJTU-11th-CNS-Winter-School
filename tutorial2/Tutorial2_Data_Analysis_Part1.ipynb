{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "### *The Tenth Computational Neuroscience Winter School*\n",
    "\n",
    "# Tutorial II: Data Analysis - Causal Inference\n",
    "---\n",
    "__Content Creator:__ Kai Chen"
   ],
   "metadata": {
    "colab_type": "text"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tutorial Objectives\n",
    "\n",
    "In this notebook, we'll learn how to do causal inferece between two time series by applying types of inference tools. More importantly, we gonna to compare performances of these methods. \n",
    "\n",
    "1. Mathematical definition of Causality and commonly used method to crack causality.\n",
    "1. Introduction to Granger Causality.\n",
    "1. Introduction to time-delayed mutual informaiton.\n",
    "1. Causal inference in linear and nonlinear systems.\n",
    "1. Causal inference for experimental data.\n"
   ],
   "metadata": {
    "colab_type": "text"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### References:\n",
    "- D. Zhou, Y. Xiao, Y. Zhang, Z. Xu and D. Cai, “Granger causality network reconstruction of conductance-based integrate-and-fire neuronal systems”, PLoS ONE, 9 (2), e87636, 2014. ([PDF](https://ins.sjtu.edu.cn/people/zdz/publication_papers/Granger_Causality_Reconstruction.pdf))\n",
    "- S. Li, Y. Xiao, D. Zhou and D. Cai, “Causal inference in nonlinear systems: Granger causality versus time-delayed mutual information”, Phys. Rev. E, 97 (5), 052216, 2018. ([PDF](https://ins.sjtu.edu.cn/people/zdz/publication_papers/Granger_causality_versus_time_delayed_mutual_information.pdf))\n",
    "- Matlab-based fast GC-estimator: [GC_clean](https://github.com/bewantbe/GC_clean)\n",
    "- Python-based mutual-information estimator: [mutual-information](https://github.com/NeoNeuron/mutual_information)\n",
    "- SciPy $\\chi^2$ statistics [documents](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chi2.html)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# Setup"
   ],
   "metadata": {
    "colab_type": "text"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# @title Import most modules and functions needed\n",
    "import time\n",
    "import numpy as np\n",
    "from scipy.stats import chi2\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['lines.linewidth'] = 2\n",
    "mpl.rcParams['legend.fontsize'] = 14\n",
    "mpl.rcParams['axes.labelsize'] = 16\n",
    "mpl.rcParams['axes.titlesize'] = 18\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Section 1: Defining and estimating causality\n",
    "**This part is borrow from Neuromatch Academay Course Material 2020.**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Let's think carefully about the statement \"**A causes B**\". To be concrete, let's take two neurons. What does it mean to say that neuron $A$ causes neuron $B$ to fire?\n",
    "\n",
    "The *interventional* definition of causality says that:\n",
    "$$\n",
    "(A \\text{ causes } B) \\Leftrightarrow ( \\text{ If we force }A \\text { to be different, then }B\\text{ changes})\n",
    "$$\n",
    "\n",
    "To determine if $A$ causes $B$ to fire, we can inject current into neuron $A$ and see what happens to $B$.\n",
    "\n",
    "**A mathematical definition of causality**: \n",
    "Over many trials, the average causal effect $\\delta_{A\\to B}$ of neuron $A$ upon neuron $B$ is the average change in neuron $B$'s activity when we set $A=1$ versus when we set $A=0$.\n",
    "\n",
    "\n",
    "$$\n",
    "\\delta_{A\\to B} = \\mathbb{E}[B | A=1] -  \\mathbb{E}[B | A=0] \n",
    "$$\n",
    "\n",
    "Note that this is an average effect. While one can get more sophisticated about conditional effects ($A$ only effects $B$ when it's not refractory, perhaps), we will only consider average effects here.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise 1: Randomized controlled trial for two neurons\n",
    "\n",
    "Let's pretend we can perform a randomized controlled trial for two neurons. Our model will have neuron $A$ synapsing on Neuron $B$:\n",
    "$$B = A + \\varepsilon$$\n",
    " where $A$ and $B$ represent the activities of the two neurons and $\\varepsilon$ is standard normal noise $\\varepsilon\\sim\\mathcal{N}(0,1)$.\n",
    "\n",
    "Our goal is to infer the mono-synaptic interactions from $A$ to $B$, i.e., the change of $B$ is induced by activity of $A$(if we ignore the effect of noise). "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def neuron_B(activity_of_A):\n",
    "    \"\"\"Model activity of neuron B as neuron A activity + noise\n",
    "\n",
    "    Args:\n",
    "    activity_of_A (ndarray): An array of shape (T,) containing the neural activity of neuron A\n",
    "\n",
    "    Returns:\n",
    "    ndarray: activity of neuron B\n",
    "    \"\"\"\n",
    "    noise = np.random.randn(activity_of_A.shape[0])\n",
    "    return activity_of_A + noise\n",
    "\n",
    "np.random.seed(2021)\n",
    "\n",
    "# Neuron A activity of zeros\n",
    "A_0 = np.zeros(50000)\n",
    "\n",
    "# Neuron A activity of ones\n",
    "A_1 = np.ones(50000)\n",
    "\n",
    "# TODO : Estimate the causal effect of A upon B\n",
    "# Use eq above (difference in mean of B when A=0 vs. A=1)\n",
    "diff_in_means = ...\n",
    "print(diff_in_means)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "However, empirically, we cannot expect the frequently appearance of the cases $A=0$ or $A=1$. Then, we may do some relexiation and modified the original expression of causality a little bit, as\n",
    "\n",
    "$$\n",
    "\\delta_{A\\to B} = \\mathbb{E}[B | A\\geq\\theta] -  \\mathbb{E}[B | A<\\theta] \n",
    "$$\n",
    "\n",
    "Note that this is an average effect. While one can get more sophisticated about conditional effects ($A$ only effects $B$ when it's not refractory, perhaps), we will only consider average effects here.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Neuron A activity of uniformly distributed iid random variables\n",
    "np.random.seed(2021)\n",
    "A = np.random.rand(50000)\n",
    "\n",
    "theta = 0.5\n",
    "# TODO : Estimate the causal effect of A upon B\n",
    "# Use eq above (difference in mean of B when A>= theta vs. A<theta)\n",
    "B = neuron_B(A)\n",
    "diff_in_means = ...\n",
    "print(f'[interacted case] delta A->B: {diff_in_means:.3e}')\n",
    "\n",
    "# comparison test: using independent\n",
    "B_indep = np.random.rand(50000)\n",
    "diff_in_means = ...\n",
    "print(f'[independent case] delta A->B: {diff_in_means:.3e}')"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# Section 2: Granger Causality"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "So far we can see that it is possible to estimate causality from experimental data, time series. And there are a mount of tools that can be used to investigate the causality, including\n",
    "- Perturbations\n",
    "- Correlations\n",
    "- Simultaneous fitting/regression\n",
    "- Instrumental variables\n",
    "- etc...\n",
    "\n",
    "All of them work well in some cases and fail in other cases. Thus, we should be very careful by keeping in mind the advantages and weaknesses of different tools of causal inference."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "And today, we are going to play with two types of model-free methods of estimating causality. Firstly, Granger causality(GC). The original paper of GC is [here](https://www.jstor.org/stable/1912791?origin=crossref&seq=1#metadata_info_tab_contents). GC is developed from the linear regression analysis, and the basic idea goes as follows.\n",
    "\n",
    "Suppose we have two time series {$x_t$} and {$y_t$}. The first thing we can do is the auto-regression analysis for the time series {$x_t$}."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$\n",
    "x_{t}=\\sum_{i=1}^{m} \\hat{a}_{i} x_{t-i}+\\hat{\\varepsilon}_{t}\n",
    "$$\n",
    "where $m$ is the regression order, $\\hat{a}_{i}$ is the regression coefficient, and $\\hat{\\varepsilon}_{t}$ is the residue after auto-regression. Similarly, we can do joint regression analysis of {$x_t$} by taking the information of time series {$y_t$} into account.\n",
    "$$\n",
    "x_{t}=\\sum_{i=1}^{m} a_{i} x_{t-i}+\\sum_{j=1}^{m} b_{j} y_{t-j}+\\varepsilon_{t}.\n",
    "$$\n",
    "Similarly, $m$ is the regression order, $a_i$, $b_j$ is the regression coefficient, and $\\varepsilon_t$ is the residual of joint regression analysis.\n",
    "\n",
    "Now it comes to the key step of GC analysis. Once you got residuals from two different regression, you may start to compare the variance of these two regression. The definition of GC from $y$ to $x$ in time domain is defined as\n",
    "\n",
    "$$\n",
    "F_{y \\rightarrow x}=\\log \\frac{\\operatorname{var}\\left(\\hat{\\varepsilon}_{t}\\right)}{\\operatorname{var}\\left(\\varepsilon_{t}\\right)}\n",
    "$$\n",
    "\n",
    "Intuitively speaking, if {$y_t$} do have causal effect onto {$x_t$}, the participation of information of $y_t$ will definitely help improve the regression analysis and lead to smaller variance of regression residual, i.e., $\\text{Var}(\\hat{\\varepsilon}_t)>\\text{Var}(\\varepsilon_t)$.\n",
    "\n",
    "On the other hand, if {$x_t$} and {$y_t$} are completely independent, the set of joint regression coefficient $b_j=0$, which leads to $\\text{Var}(\\hat{\\varepsilon}_t)=\\text{Var}(\\varepsilon_t)$. Therefore, $F_{y\\rightarrow x}=0$.\n",
    "\n",
    "Now, let's start to build up codes for estimating GC and play with some generated data.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 1: Create structure array for regression analysis.\n",
    "\n",
    "Rewrite the auto regression model into matrix form as\n",
    "$$\n",
    "\\mathbf{x}_{t}= \\mathbf{X}\\mathbf{a}+\\mathbf{\\hat{\\varepsilon}_t}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{X}_{(t-m)\\times m} = \\left[\\mathbf{x}_{t-1}, \\mathbf{x}_{t-2}, \\cdots, \\mathbf{x}_{t-m}\\right]$, and $\\mathbf{a} = \\left[\\hat{a}_1, \\hat{a}_2, \\cdots, \\hat{a}_m\\right]^T$. Here, $\\mathbf{X}$ is the structure array that we want in the regression model. \n",
    "\n",
    "Similarly for joint regression case,\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_{t}= \\mathbf{Z}\\mathbf{p}+\\mathbf{\\varepsilon_t}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{Z}_{(t-m)\\times 2m} = \\left[\\mathbf{x}_{t-1}, \\mathbf{x}_{t-2}, \\cdots, \\mathbf{x}_{t-m}, \\mathbf{y}_{t-1}, \\mathbf{y}_{t-2}, \\cdots, \\mathbf{y}_{t-m}\\right]$, and $\\mathbf{p} = \\left[a_1, a_2, \\cdots, a_m, b_1, \\cdots, b_m\\right]^T$.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def create_structure_array(x:np.ndarray, order:int)->np.ndarray:\n",
    "    '''\n",
    "    Prepare structure array for regression analysis.\n",
    "    \n",
    "    Args:\n",
    "    x         : original time series\n",
    "    order     : regression order\n",
    "    \n",
    "    Return:\n",
    "    x_array   : structure array with shape (len(x)-order) by (order).\n",
    "    \n",
    "    '''\n",
    "    # TODO: finish the function\n",
    "    N = len(x) - order\n",
    "    x_array = np.zeros((N, order))\n",
    "    for i in range(order):\n",
    "        x_array[:, i] = ...\n",
    "    return x_array"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# test create_structure_array\n",
    "test_X = np.random.randn(10)\n",
    "X_array = create_structure_array(test_X, 3)\n",
    "print(test_X)\n",
    "print(X_array)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 2: Solve regression problem\n",
    "After you prepare the structure array, then we need to solve the regression problem. Directly from the matrix expression of regression model, we can treat it as an optimization problem or least square problem.\n",
    "\n",
    "$$\n",
    "\\min_\\mathbf{a} \\left\\lVert\\mathbf{x}_{t} - \\mathbf{X}\\mathbf{a}\\right\\rVert_2\n",
    "$$\n",
    "\n",
    "Theoretically, we can calculate it using the expression below\n",
    "$$\n",
    "\\hat{\\mathbf{a}}  = \\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1} \\mathbf{X}^T\\mathbf{x}_t\n",
    "$$\n",
    "\n",
    "Numerically, in order to by pass the direct matrix inverse for large matrix, here we use the least square solver in numpy, named as `numpy.linalg.lstsq()`, which use SVD to accelerate the numerical solver of least square problem.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def auto_reg(x, order)->np.ndarray:\n",
    "    '''\n",
    "    Auto regression analysis of time series.\n",
    "    \n",
    "    Args:\n",
    "    x         : original time series\n",
    "    order     : regression order\n",
    "    \n",
    "    Return:\n",
    "    res       : residual vector\n",
    "    \n",
    "    '''\n",
    "    # TODO: finish the function\n",
    "    # hint: using np.linalg.lstsq() to solve least square problem\n",
    "    reg_array = create_structure_array(x, order)\n",
    "    \n",
    "    # Add your code here\n",
    "    \n",
    "    res = ...\n",
    "    \n",
    "    return res"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# test auto_reg()\n",
    "test_x = np.random.randn(10)\n",
    "print(auto_reg(test_x, 3))"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def joint_reg(x, y, order)->np.ndarray:\n",
    "    '''\n",
    "    Joint regression analysis of time series.\n",
    "    \n",
    "    Args:\n",
    "    x         : original time series 1\n",
    "    y         : original time series 2\n",
    "    order     : regression order\n",
    "    \n",
    "    Return:\n",
    "    res       : residual vector\n",
    "    \n",
    "    '''\n",
    "    # TODO: finish the function\n",
    "    reg_array_x = create_structure_array(x, order)\n",
    "    reg_array_y = create_structure_array(y, order)\n",
    "    \n",
    "    # Add your code here\n",
    "    \n",
    "    res = ...\n",
    "    \n",
    "    return res"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# test joint_reg()\n",
    "test_x = np.random.randn(10)\n",
    "test_y = np.random.randn(10)\n",
    "print(joint_reg(test_x, test_y, 3))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 3: Calculate GC\n",
    "\n",
    "Using the regression model defined above to calculate GC value."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def GC(x, y, order):\n",
    "    '''\n",
    "    Granger Causality from y to x\n",
    "    \n",
    "    Args:\n",
    "    x         : original time series (dest)\n",
    "    y         : original time series (source)\n",
    "    order     : regression order\n",
    "    \n",
    "    Return:\n",
    "    GC_value  : residual vector\n",
    "    \n",
    "    '''\n",
    "    res_auto = auto_reg(x, order)\n",
    "    res_joint = joint_reg(x, y, order)\n",
    "    GC_value = 2.*np.log(res_auto.std()/res_joint.std())\n",
    "    return GC_value\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_X = np.random.randn(100000)\n",
    "test_Y = np.random.randn(100000)\n",
    "test_Y[1:] += test_X[:-1]*0.5\n",
    "print(f'GC Y->X: {GC(test_X, test_Y, 100):.3e}')\n",
    "print(f'GC X->Y: {GC(test_Y, test_X, 100):.3e}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 4: Determin the significance level of GC value.\n",
    "\n",
    "Suppose time seies $x_t$ and $y_t$ do not have any causal relationship, then theoretically speaking, the GC value between them $F_{x\\rightarrow y}$ = $F_{y\\rightarrow x}$ = 0. However, due to the finite data length and finite regression order, the empirical value of GC $\\tilde{F}_{x\\rightarrow y}$ and $\\tilde{F}_{y\\rightarrow x}$ asymtotically follows $\\chi^2$ distribution, \n",
    "\n",
    "$$l\\tilde{F}_{x\\rightarrow y},l\\tilde{F}_{y\\rightarrow x}\\sim \\chi^2(m),$$\n",
    "\n",
    "where $l$ is the data length and $m$ is the regression order.\n",
    "\n",
    "Using this as the null-hypothesis, we can calculate the significance level (threshold) to reject null-hypothesis with given $p$-value."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def GC_SI(p, order, length):\n",
    "    '''\n",
    "    Significant level of GC value.\n",
    "    \n",
    "    Args\n",
    "    p       : p-value\n",
    "    order   : parameter of chi^2 distribution\n",
    "    length  : length of data.\n",
    "    \n",
    "    Return:\n",
    "    significant level of null hypothesis (GC \n",
    "        between two independent time seies)\n",
    "    \n",
    "    '''\n",
    "    return chi2.ppf(1-p, order)/length"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "length = 1e5\n",
    "order = 10\n",
    "p = 0.0001\n",
    "print(f'GC threshold for p={p:.1e}, length={length:.1e}, order={order:d}: \\\n",
    "      {GC_SI(p, order, length):.3e}')"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Section 3: Time-delayed Mutual Information (TDMI)\n",
    "\n",
    "TDMI is an information-theoretic approach for detecting causal interactions. In general, the quantity of mutual information characterizes the common information shared between two signals. Given two stationary signals X and Y with their time traces {$x_t$} and {$y_t$}, respectively, the mutual information between them is defined as\n",
    "$$\n",
    "I(X, Y)=\\sum_{x_{t}} \\sum_{y_t} p\\left(x_{t}, y_{t}\\right) \\log \\frac{p\\left(x_{t}, y_{t}\\right)}{p\\left(x_{t}\\right) p\\left(y_{t}\\right)}\n",
    "$$\n",
    "where $ p(xt ,yt )$ is the joint probability distribution of $X = x_t$ and $Y = y_t$ , $p(x_t)$ and $p(y_t)$ are their marginal probability distributions. In particular, $I (X,Y) = 0$ is equivalent to $p(x_t ,y_t) = p(x_t)p(y_t)$, which indicates that signals $X$ and $Y$ are independent if they do not share information.\n",
    "\n",
    "Mutual information is symmetric, i.e., $I (X,Y ) = I (Y,X)$. Therefore, it cannot be applied directly to infer the direction of interactions between two signals. To overcome this limitation, one can introduce a time-lag parameter $\\tau$ to capture the delay of information transfer between the two signals. TDMI as a function of time-lag $\\tau$ is defined as\n",
    "\n",
    "$$\n",
    "I(X, Y, \\tau)=\\sum_{x_{t}} \\sum_{y_{t-\\tau}} p\\left(x_{t}, y_{t-\\tau}\\right) \\log \\frac{p\\left(x_{t}, y_{t-\\tau}\\right)}{p\\left(x_{t}\\right) p\\left(y_{t-\\tau}\\right)}\n",
    "$$\n",
    "\n",
    "where $p(x_t ,y_{t −\\tau} )$ is the joint probability distribution of $X = x_t$ and $Y = y_{t −\\tau}$ . A nonzero amplitude of the mutual information as a function of $\\tau$ indicates the existence of interactions between two signals; the sign of the time-lag $\\tau$ where $I(X,Y,\\tau)$ reaches its peak magnitude is used to infer the information flow direction that can be further interpreted as the causal direction of interaction. A negative $\\tau$ indicates that $X$ shares a maximum amount of information with the future of $Y$ , thus $X$ drives $Y$. A positive $\\tau$ indicates that $X$ shares a maximum amount of information with the past of $Y$, thus $X$ is driven by $Y$ ."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Add you code to estimate mutual information\n",
    "def mi(x, y, bins = None):\n",
    "    \"\"\"mutual information for 0-1 binary time series\n",
    "    :param x: first series\n",
    "    :param y: second series\n",
    "    :return: mutual information\n",
    "\n",
    "    \"\"\"\n",
    "    # TODO: finish the function\n",
    "    # hint: using numpy.histogram2d() to help you with the estimation of joint pdf\n",
    "    \n",
    "    # ADD your code here\n",
    "    \n",
    "    mi_val = ...\n",
    "    \n",
    "    return mi_val\n",
    "\n",
    "def dmi(x, y, delay):\n",
    "    if delay == 0:\n",
    "        return mi(x, y)\n",
    "    elif delay < 0:\n",
    "        return mi(x[-delay:],y[:delay])\n",
    "    elif delay > 0:\n",
    "        return mi(x[:-delay],y[delay:])\n",
    "\n",
    "def tdmi(x, y, time_range):\n",
    "    return np.array([dmi(x,y,delay) for delay in time_range])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# test mi()\n",
    "# test dmi()\n",
    "# test tdmi()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### [Python code for mutual information estimation](https://github.com/NeoNeuron/mutual_information)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# install minfo, a Cython-based mutual information estimator;\n",
    "!pip install minfo"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that: if `ImportError` occurs later, please reinstall `numpy` with conda instead of `pip`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# import mutual information estimator\n",
    "from minfo.mi_float import tdmi_omp\n",
    "# wrap funciotns estimating time-delayed mutual information\n",
    "def TDMI(x:np.ndarray, y:np.ndarray, time_range:np.ndarray) -> np.ndarray:\n",
    "\n",
    "    assert 0 in delay\n",
    "    assert time_range.dtype == int\n",
    "\n",
    "    tdmi_val = np.zeros_like(time_range, dtype=float)\n",
    "    pos_end = np.max(time_range).astype(int)\n",
    "    neg_end = -np.min(time_range).astype(int)\n",
    "    if neg_end == 0:\n",
    "        tdmi_val = tdmi_omp(x, y, pos_end+1)\n",
    "    elif pos_end == 0:\n",
    "        tdmi_val = np.flip(tdmi_omp(y, x, neg_end+1))\n",
    "    else:\n",
    "        tdmi_val[neg_end:] = tdmi_omp(x, y, pos_end+1)\n",
    "        tdmi_val[:neg_end+1] = np.flip(tdmi_omp(y, x, neg_end+1))\n",
    "    \n",
    "    return tdmi_val"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# test TDMI()\n",
    "test_x = np.random.randn(100000)\n",
    "test_y = np.random.randn(100000)\n",
    "test_y[1:] += test_x[:-1]*0.5\n",
    "delay = np.arange(21)-10\n",
    "plt.plot(delay, TDMI(test_x, test_y, delay))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# Section 4: Causal inference in linear and nonlinear systems.\n",
    "\n",
    "Now use these two methods you code above to play with few artifitially generated time series."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Case A: Unidirection misinferred as no interaction\n",
    "\\begin{array}{l}x_{t}=\\varepsilon_{t} \\\\ y_{t}=-0.1 x_{t-1}+\\eta_{t}\\end{array}"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def system_A(length):\n",
    "    # Add your code here\n",
    "    x = ...\n",
    "    y = ...\n",
    "\n",
    "    return x, y"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# GC version\n",
    "length = int(1e7)\n",
    "order = 10\n",
    "X, Y = system_A(length)\n",
    "print(f'GC Y->X     : {GC(X, Y, order):.3e}')\n",
    "print(f'GC X->Y     : {GC(Y, X, order):.3e}')\n",
    "print(f'GC threshold: {GC_SI(0.001, order, length):.3e}')\n"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# TDMI version\n",
    "delay = np.arange(21)-10\n",
    "plt.plot(delay, TDMI(X,Y,delay))"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Nonlinear transformation\n",
    "$$\n",
    "\\tilde{x}_t = x_t^2\n",
    "$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# GC version\n",
    "X_tilde = X**2\n",
    "print(f'GC Y->X     : {GC(X_tilde, Y, order):.3e}')\n",
    "print(f'GC X->Y     : {GC(Y, X_tilde, order):.3e}')\n",
    "print(f'GC threshold: {GC_SI(0.001, order, length):.3e}')"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# TDMI version\n",
    "delay = np.arange(21)-10\n",
    "plt.plot(delay, TDMI(X_tilde,Y,delay))"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Case B: Unidirection misinferred as bidirection\n",
    "\\begin{array}{l}x_{t}=-0.3 x_{t-1}+\\varepsilon_{t} \\\\ y_{t}=0.3 y_{t-1}-0.9 x_{t-1}+\\eta_{t}\\end{array}"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def system_B(length):\n",
    "    # Add your code here\n",
    "    x = ...\n",
    "    y = ...\n",
    "\n",
    "    return x, y"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# GC version\n",
    "length = int(1e7)\n",
    "order = 10\n",
    "X, Y = system_B(length)\n",
    "print(f'GC Y->X     : {GC(X, Y, order):.3e}')\n",
    "print(f'GC X->Y     : {GC(Y, X, order):.3e}')\n",
    "print(f'GC threshold: {GC_SI(0.001, order, length):.3e}')\n"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# TDMI version\n",
    "delay = np.arange(21)-10\n",
    "plt.plot(delay, TDMI(X,Y,delay))"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Nonlinear transformation\n",
    "$$\n",
    "\\tilde{x}_t = \\left[\\frac{x_t+|x_t|}{2}\\right]^5\n",
    "$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# GC version\n",
    "X_tilde = ((X+np.abs(X))/2)**5\n",
    "print(f'GC Y->X     : {GC(X_tilde, Y, order):.3e}')\n",
    "print(f'GC X->Y     : {GC(Y, X_tilde, order):.3e}')\n",
    "print(f'GC threshold: {GC_SI(0.001, order, length):.3e}')\n"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# TDMI version\n",
    "delay = np.arange(21)-10\n",
    "plt.plot(delay, TDMI(X_tilde,Y,delay))"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Case C: Unidirection misinferred as reversed unidirection\n",
    "\\begin{aligned}\n",
    "&x_{t}=-\\sum_{k=1}^{8} c_{k} x_{t-k}+\\varepsilon_{t} \\\\\n",
    "&y_{t}=-\\sum_{k=1}^{8} c_{k} y_{t-k}+100 \\sum_{k=1}^{9} c_{k-1} x_{t-k}+\\eta_{t}\n",
    "\\end{aligned}"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def system_C(length):\n",
    "    # Add your code here\n",
    "    x = ...\n",
    "    y = ...\n",
    "    \n",
    "    return x, y"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# GC version\n",
    "length = int(1e7)\n",
    "order = 10\n",
    "X, Y = system_C(length)\n",
    "print(f'GC Y->X     : {GC(X, Y, order):.3e}')\n",
    "print(f'GC X->Y     : {GC(Y, X, order):.3e}')\n",
    "print(f'GC threshold: {GC_SI(0.001, order, length):.3e}')"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# TDMI version\n",
    "delay = np.arange(21)-10\n",
    "plt.plot(delay, TDMI(X,Y,delay))"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Nonlinear transformation\n",
    "$$\n",
    "\\tilde{x}_t = x_t^5\n",
    "$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# GC version\n",
    "X_tilde = X**5\n",
    "print(f'GC Y->X     : {GC(X_tilde, Y, order):.3e}')\n",
    "print(f'GC X->Y     : {GC(Y, X_tilde, order):.3e}')\n",
    "print(f'GC threshold: {GC_SI(0.001, order, length):.3e}')"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# TDMI version\n",
    "delay = np.arange(21)-10\n",
    "plt.plot(delay, TDMI(X_tilde,Y,delay))"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Case D: Bidirection misinferred as no interaction\n",
    "\\begin{array}{l}x_{t}=-0.1 y_{t-1}+\\varepsilon_{t} \\\\ y_{t}=-0.1 x_{t-1}+\\eta_{t}\\end{array}"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def system_D(length):\n",
    "    # Add your code here\n",
    "    x = ...\n",
    "    y = ...\n",
    "    \n",
    "    return x, y"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# GC version\n",
    "length = int(1e7)\n",
    "order = 10\n",
    "X, Y = system_D(length)\n",
    "print(f'GC Y->X     : {GC(X, Y, order):.3e}')\n",
    "print(f'GC X->Y     : {GC(Y, X, order):.3e}')\n",
    "print(f'GC threshold: {GC_SI(0.001, order, length):.3e}')"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# TDMI version\n",
    "delay = np.arange(21)-10\n",
    "plt.plot(delay, TDMI(X,Y,delay))"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Nonlinear transformation\n",
    "$$\n",
    "\\tilde{x}_t = x_t^2\n",
    "$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# GC version\n",
    "X_tilde = X**2\n",
    "print(f'GC Y->X     : {GC(X_tilde, Y, order):.3e}')\n",
    "print(f'GC X->Y     : {GC(Y, X_tilde, order):.3e}')\n",
    "print(f'GC threshold: {GC_SI(0.001, order, length):.3e}')"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# TDMI version\n",
    "delay = np.arange(21)-10\n",
    "plt.plot(delay, TDMI(X_tilde,Y,delay))"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Case E: Bidirection misinferred as unidirection\n",
    "\\begin{aligned}\n",
    "x_{t}=-\\sum_{k=1}^{8} c_{k} x_{t-k}+0.5 \\sum_{k=1}^{9} c_{k-1} y_{t-k}+\\varepsilon_{t} \\\\\n",
    "y_{t}=-\\sum_{k=1}^{8} c_{k} y_{t-k}+0.5 \\sum_{k=1}^{9} c_{k-1} x_{t-k}+\\eta_{t}\n",
    "\\end{aligned}"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def system_E(length):\n",
    "    # Add your code here\n",
    "    x = ...\n",
    "    y = ...\n",
    "    \n",
    "    return x, y"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# GC version\n",
    "length = int(1e2)\n",
    "order = 10\n",
    "X, Y = system_E(length)\n",
    "print(f'GC Y->X     : {GC(X, Y, order):.3e}')\n",
    "print(f'GC X->Y     : {GC(Y, X, order):.3e}')\n",
    "print(f'GC threshold: {GC_SI(0.001, order, length):.3e}')"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# TDMI version\n",
    "delay = np.arange(21)-10\n",
    "plt.plot(delay, TDMI(X,Y,delay))"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Nonlinear transformation\n",
    "$$\n",
    "\\tilde{x}_t = \\tanh(10x_t)\n",
    "$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# GC version\n",
    "X_tilde = np.tanh(10*X)\n",
    "print(f'GC Y->X     : {GC(X_tilde, Y, order):.3e}')\n",
    "print(f'GC X->Y     : {GC(Y, X_tilde, order):.3e}')\n",
    "print(f'GC threshold: {GC_SI(0.001, order, length):.3e}')"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# TDMI version\n",
    "delay = np.arange(21)-10\n",
    "plt.plot(delay, TDMI(X_tilde,Y,delay))"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Summary Table\n",
    "![](./summary_table.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# Section 5: Interactions between neuronal signals"
   ],
   "metadata": {
    "colab_type": "text"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from scipy.io import loadmat\n",
    "data = loadmat('lab02-xk128-103005006-f.mat')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "spk_train = data['scsig053ats'][0,0][-1]\n",
    "# lfp = data['AD27theta_normalized_f_ad_000'][0,0][-1]\n",
    "lfp = data['AD27_ad_000'][0,0][-1]\n",
    "\n",
    "t_start = 170   # unit ms\n",
    "t_end   = 270   # unit ms\n",
    "\n",
    "dt = 0.001\n",
    "lfp_theta = lfp[int(t_start/dt):int(t_end/dt)].flatten()\n",
    "spike = spk_train[(spk_train>=t_start)&(spk_train<t_end)] - t_start"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def spike2bool(spike_train, length, dt):\n",
    "    '''\n",
    "    Transform spike train data to binary time series.\n",
    "    \n",
    "    Args:\n",
    "    spike_train : original spike train\n",
    "    length      : length of desired binary series\n",
    "    dt          : size of time step\n",
    "    \n",
    "    Return:\n",
    "    binary time series of spiking time series\n",
    "    '''\n",
    "    spike01 = np.zeros(length, dtype=int)\n",
    "    spike01[np.floor(spike_train/dt).astype(int)] = 1\n",
    "    return spike01"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "spike01 = spike2bool(spike, len(lfp_theta), dt)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "delay = np.arange(1001)-500\n",
    "plt.plot(delay, tdmi(spike01, lfp_theta, delay), 'navy')\n",
    "plt.axvline(0, color='orange')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# GC analysis\n",
    "order = 30\n",
    "print(f'GC lfp->spike : {GC(spike01, lfp_theta, order):.3e}')\n",
    "print(f'GC spike->lfp : {GC(lfp_theta, spike01, order):.3e}')\n",
    "print(f'GC threshold  : {GC_SI(0.001, order, len(lfp_theta)):.3e}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# End of the tutorial"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "W8_Tutorial2",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}